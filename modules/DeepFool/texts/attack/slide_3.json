{
    "info": "   Результат 'свёртки', проведенной CNN на предыдущем этапе, передается функции активации, которая представляет собой некую нелинейную функцию.\n   Выбор функции завивит от разработчика, однако в большинстве современных CNN сетей используется функция ReLU (Rectified Linear Unit).\n   Алгоритм атаки Deep Fool в свою очередь находит ближайшую гиперплоскость границы двух классов распознавания и сдвигает исходное изображение x в сторону вектора, ортогонального такой гиперплоскости.\n   В данной демонстрации зашумленность изображения преувеличена для наглядности, в реальной ситуации оно практически не заметно человеческому взгляду, однако положим проведенное зашумление оптимальным.",

    "script": "[+] ACTIVATION LAYER: Analysing layer data...",

    "action": {}
}