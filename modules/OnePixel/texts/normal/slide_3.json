{
	"info": "   Результат 'свёртки', проведенной на предыдущем этапе, передается функции активации, которая представляет собой некую нелинейную функцию.\n   Выбор функции завивит от разработчика, однако в большинстве современных CNN сетей используется функция ReLU (Rectified Linear Unit). По сути ReLU это операция отсечения отрицательной части скалярной величины.\n   Простота этой функции значительно ускоряет процесс классификации.",

    "script": "[+] ACTIVATION LAYER: Analysing layer data...",

    "action": {}
}